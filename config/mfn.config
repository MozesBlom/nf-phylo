/*
 * -------------------------------------------------
 *  Nextflow config file for running Polish my Reads! on MfN cluster
 * -------------------------------------------------
 *   Use as follows:
 *   nextflow run main.nf -profile mfn
 */

executor {
    name = 'slurm'
    queueSize = 200
    submitRateLimit = '1sec'
}

process {
    config_profile_name = 'MfN cluster profile'
    config_profile_description = 'Profile that works well with the Museum f√ºr Naturkunde cluster'
    executor = 'slurm'
    queue = 'standard'
    cpus = 1
    memory = '8 GB'
    time = '8h'
    conda = '/path/to/envs/nf-phylo'

    // This pipeline can be computationally intensive and needs to be adjusted by genome and sampling.
    // The settings below work for a 50 individual avian genomics project.
    // Please note that if you change the label, the nextflow script itself will also need to be adjusted.
    // Each computationally intensive process has been tagged with a guesstimate of requirements. I've used
    // a labeling scheme so they can be adjusted based on genome/dataset requirements.
    // The labeling code below refers to: H(igh)/I(ntermediate)/S(mall), C(ores)/M(em)/T(ime). In example:
    // HC_IM_ST = High number of cores, Intermediate amount of RAM and small amount of time requested.

    withLabel: 'SC_SM_IT' {
        cpus = 2
        memory = '16 GB'
        time = '7d'
    }

    withLabel: 'SC_SM_HT' {
        cpus = 2
        memory = '16 GB'
        time = '28d'
    }

    withLabel: 'SC_IM_ST' {
        cpus = 2
        memory = '48 GB'
        time = '24h'
    }

    withLabel: 'SC_IM_IT' {
        cpus = 2
        memory = '48 GB'
        time = '7d'
    }

    withLabel: 'IC_IM_HT' {
        cpus = 16
        memory = '48 GB'
        time = '28d'
    }

    withLabel: 'SC_HM_IT' {
        cpus = 2
        memory = '96 GB'
        time = '7d'
    }

    withLabel: 'IC_HM_HT' {
        cpus = 16
        memory = '96 GB'
        time = '28d'
    }

}
